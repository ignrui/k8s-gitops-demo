# Informe de Laboratorio DevOps: PrÃ¡ctica de GitOps con Kubernetes y ArgoCD

Este laboratorio tiene como objetivos:
- Introducir a los principios de GitOps en kubernetes.
- ConfiguraciÃ³n dee Minikube y ArgoCD
- Configuraciones declaratibas de kubernetes.
- Uso de overlays
- AutomatizaciÃ³n de despliegues y promociÃ³n mediante pull requests


## Fase 1: ConfiguraciÃ³n del Entorno

### 1.1 InstalaciÃ³n de Herramientas Base
Durante la practica he trabajado desde el entorno wsl2 con dockers.
se instalaron las siguientes herramientas:
- **Minikube**: Crea un clÃºster Kubernetes local para desarrollo y pruebas
- **kubectl**: Cliente oficial para interactuar con la API de Kubernetes

```
curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 && sudo install minikube-linux-amd64 /usr/local/bin/minikube
sudo apt-get update && sudo apt-get install -y kubectl
```
### 1.2 InicializaciÃ³n del ClÃºster Kubernetes
  
**Comando ejecutado:**
```bash
minikube start --driver=docker --cpus=4 --memory=4096 --disk-size=20g
```

**ExplicaciÃ³n detallada:**
- `--driver=docker`: Usa Docker como hypervisor (mÃ¡s eficiente que VM)
- `--cpus=4 --memory=4096`: Asigna recursos suficientes para ArgoCD y aplicaciones
- `--disk-size=20g`: Espacio para imÃ¡genes de contenedores y datos persistentes

**Resultado:** ClÃºster Kubernetes corriendo localmente

### 1.3 HabilitaciÃ³n de Addons Esenciales

**Comandos ejecutados:**
```bash
minikube addons enable metrics-server
minikube addons enable ingress
```

**Â¿Para quÃ© sirven?**
- **metrics-server**: Recolecta mÃ©tricas de CPU/memoria de pods y nodos
- **ingress**: Controlador para gestionar trÃ¡fico HTTP/HTTPS hacia servicios

---

## Fase 2: InstalaciÃ³n y ConfiguraciÃ³n de ArgoCD

### 2.1 Despliegue de ArgoCD

**Comandos ejecutados:**
```bash
kubectl create namespace argocd
kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml
kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=argocd-server -n argocd --timeout=300s
```

**Â¿QuÃ© hace cada comando?**
1. Crea un namespace dedicado para aislar ArgoCD
2. Despliega todos los componentes de ArgoCD (servidor, controlador, repo-server, etc.)
3. Espera hasta que el servidor estÃ© listo antes de continuar

**Componentes instalados:**
- `argocd-server`: Interfaz web y API REST
- `argocd-application-controller`: Gestiona el estado de las aplicaciones
- `argocd-repo-server`: Clona y procesa repositorios Git
- `argocd-dex-server`: Servidor de autenticaciÃ³n
- `argocd-redis`: Cache para mejorar rendimiento

### 2.2 Acceso a ArgoCD

**Comandos ejecutados:**
```bash
kubectl port-forward svc/argocd-server -n argocd 8080:443 > /dev/null 2>&1 &
# Note: This runs in the background. To stop it later, find the process:
# ps aux | grep "port-forward"
# kill <PID>
kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 -d
```

**ExplicaciÃ³n:**
- Port-forward permite acceder a ArgoCD desde localhost:8080
- La contraseÃ±a inicial se genera automÃ¡ticamente y se almacena en un secreto
- Credenciales: `admin` / `1DRl3XUEVzeSRA47` 

Ya podemos aceder a argoCD en `localhost:8080`, y relizar el cambio de contraseÃ±a
---

## Fase 3: Estructura del Repositorio GitOps
Creamos el repositorio `k8s-gitops-demo` en github y lo clonamos en local para empezar a trabajar sobre Ã¡l.

### 3.1 Arquitectura Base con Kustomize

La estructura implementada sigue el patrÃ³n "base + overlays", con unas carpetas commo las mostradas a continuaciÃ³n:

```
k8s-gitops-demo/
â”œâ”€â”€ base/                    # ConfiguraciÃ³n comÃºn a todos los entornos
â”‚   â”œâ”€â”€ deployment.yaml      # DefiniciÃ³n de la aplicaciÃ³n
â”‚   â”œâ”€â”€ service.yaml         # ExposiciÃ³n del servicio
â”‚   â””â”€â”€ kustomization.yaml   # ConfiguraciÃ³n de Kustomize
â”œâ”€â”€ overlays/                # Personalizaciones por entorno
â”‚   â”œâ”€â”€ dev/
â”‚   â”œâ”€â”€ staging/
â”‚   â””â”€â”€ prod/
â””â”€â”€ argocd-app-*.yaml       # Definiciones de aplicaciones ArgoCD
```

### 3.2 AnÃ¡lisis Detallado de Archivos Base

#### `base/deployment.yaml`
```yaml
apiVersion: apps/v1  # VersiÃ³n de API para Deployments
kind: Deployment  # Tipo de recurso Kubernetes (deployment es conjunto de pods, otros posibles son Service, ConfigMap, Secret)
metadata:
  name: webapp  # Nombre Ãºnico del deployment
spec:
  replicas: 2  # NÃºmero de pods idÃ©nticos a ejecutar
  template:  # Plantilla para crear los pods
    spec:
      containers:
      - name: webapp  # Nombre del contenedor, para identificarlo en otros recursos
        image: nginxdemos/hello:latest  # Imagen Docker (evita :latest en prod)
        resources:
          requests:  # Recursos GARANTIZADOS por Kubernetes
            memory: "64Mi"  # RAM mÃ­nima reservada
            cpu: "50m"  # CPU mÃ­nima reservada (50m = 5% de 1 core)
          limits:  # Recursos MÃXIMOS permitidos
            memory: "128Mi"  # Si excede: pod es terminado (OOMKilled)
            cpu: "100m"  # Si excede: es ralentizado (throttled)
        livenessProbe:  # Â¿EstÃ¡ vivo? Si falla â†’ reinicia el contenedor
          httpGet:
            path: /  # Endpoint a verificar (debe devolver 200-399)
            port: 80  # Puerto donde verificar
        readinessProbe:  # Â¿Listo para trÃ¡fico? Si falla â†’ quita del load balancer
          httpGet:
            path: /  # Endpoint para verificar disponibilidad
            port: 80  # Puerto donde verificar
```

**Elementos clave:**
- **Image**: `nginxdemos/hello` - aplicaciÃ³n demo que muestra informaciÃ³n del contenedor
- **Resources**: Limita uso de CPU/memoria para evitar consumo excesivo
- **Probes**: Verifican que la aplicaciÃ³n estÃ© viva y lista para recibir trÃ¡fico

#### `base/service.yaml`
```yaml
apiVersion: v1  # API bÃ¡sica de Kubernetes (v1, no apps/v1)
kind: Service  # Tipo de recurso: punto de acceso a los pods
metadata:
  name: webapp-service  # Nombre Ãºnico del servicio
spec:
  selector:  # Selector: encuentra los pods por sus etiquetas
    app: webapp  # Busca todos los pods con label "app=webapp"
  ports:
  - port: 80  # Puerto del SERVICE (donde otros pods se conectan)
    targetPort: 80  # Puerto del POD (donde escucha el contenedor)
  type: ClusterIP  # Tipo: acceso solo INTERNO al cluster (por defecto)
```
Este Service es el "balanceador de carga interno" que permite acceder a los pods del Deployment de forma estable. 

**FunciÃ³n:**
- Expone los pods internamente en el clÃºster
- `ClusterIP`: Solo accesible desde dentro del clÃºster
- El selector conecta con pods que tengan la etiqueta `app: webapp`

#### `base/kustomization.yaml`
```yaml
apiVersion: kustomize.config.k8s.io/v1beta1  # API de Kustomize
kind: Kustomization  # Tipo: archivo de configuraciÃ³n Kustomize
resources:  # Lista de archivos YAML a gestionar
  - deployment.yaml  # Incluye el archivo deployment
  - service.yaml     # Incluye el archivo service
commonLabels:  # Etiquetas que se aÃ±aden AUTOMÃTICAMENTE a todos los recursos
  app: webapp           # AÃ±ade label "app: webapp" a deployment Y service
  managed-by: argocd    # AÃ±ade label "managed-by: argocd" a ambos
```

**PropÃ³sito:**
- Agrupa recursos relacionados
- AÃ±ade etiquetas comunes automÃ¡ticamente
- Base para personalizaciones especÃ­ficas por entorno

### 3.3 Overlays por Entorno

#### Ejemplo: `overlays/dev/kustomization.yaml`
```yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
namespace: dev
namePrefix: dev-
bases:
  - ../../base # Carga las kustomizations
patchesStrategicMerge:
  - replica-patch.yaml #Le aplica este parche
commonLabels:
  environment: dev
```

**Transformaciones aplicadas:**
1. **namespace: dev** â†’ Despliega en namespace especÃ­fico
2. **namePrefix: dev-** â†’ Recursos se llaman `dev-webapp`, `dev-webapp-service`
3. **patchesStrategicMerge** â†’ Aplica modificaciones especÃ­ficas
4. **commonLabels** â†’ AÃ±ade etiqueta de entorno

#### `overlays/dev/replica-patch.yaml`
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webapp
spec:
  replicas: 1  # Solo 1 rÃ©plica en desarrollo
```

**LÃ³gica por entorno:**
- **dev**: 1 rÃ©plica (recursos limitados, pruebas rÃ¡pidas)
- **staging**: 2 rÃ©plicas (simula producciÃ³n)
- **prod**: 3 rÃ©plicas (alta disponibilidad)

>En este punto se hizo commit y push al repo

---

## ğŸš€ Fase 4: Aplicaciones ArgoCD

### 4.1 DefiniciÃ³n de AplicaciÃ³n para Desarrollo

#### `argocd-app-dev.yaml`
```yaml
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: webapp-dev
  namespace: argocd
spec:
  source:
    repoURL: https://github.com/ignrui/k8s-gitops-demo
    targetRevision: main
    path: overlays/dev
  destination:
    server: https://kubernetes.default.svc
    namespace: dev
  syncPolicy:
    automated:
      prune: true       # Borra recursos eliminados del Git
      selfHeal: true    # Revierte cambios manuales
      allowEmpty: false
    syncOptions:
      - CreateNamespace=true
```

**CaracterÃ­sticas clave:**
- **SincronizaciÃ³n automÃ¡tica**: Detecta cambios en Git y los aplica automÃ¡ticamente
- **prune: true**: Si borramos un archivo de Git, ArgoCD borra el recurso del clÃºster
- **selfHeal: true**: Si modificamos algo manualmente en Kubernetes, ArgoCD lo revierte
- **CreateNamespace=true**: Crea el namespace automÃ¡ticamente si no existe

### 4.2 AplicaciÃ³n para ProducciÃ³n

La aplicaciÃ³n de producciÃ³n (`argocd-app-prod.yaml`) difiere en:
- **SincronizaciÃ³n manual**: Requiere aprobaciÃ³n explÃ­cita para cambios
- **path: overlays/prod**: Usa configuraciÃ³n especÃ­fica de producciÃ³n
- **namespace: prod**: Despliega en entorno aislado

**Â¿Por quÃ© manual en producciÃ³n?**
- Mayor control sobre cambios crÃ­ticos
- Permite revisiones y validaciones antes del despliegue
- Reduce riesgo de interrupciones de servicio

---

## âš™ï¸ Fase 5: Despliegue y OperaciÃ³n


## ğŸ”„ Fase 5: Experimentar GitOps en AcciÃ³n

### 5.1 Escalado de rÃ©plicas en Dev y sincronizaciÃ³n automÃ¡tica

**AcciÃ³n:** Se edita `overlays/dev/replica-patch.yaml` para escalar a 2 rÃ©plicas y se hace commit/push a GitHub.

**Comandos:**
```bash
cat > overlays/dev/replica-patch.yaml << 'EOF'
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webapp
spec:
  replicas: 2  # Scale up dev environment
EOF

git add overlays/dev/replica-patch.yaml
git commit -m "Scale dev environment to 2 replicas"
git push origin main
```

**ExplicaciÃ³n:** Al modificar el nÃºmero de rÃ©plicas y subir el cambio a Git (y crear el pull request con merge a main), ArgoCD sincroniza automÃ¡ticamente el clÃºster. De manera automÃ¡tica, se crea un segundo pod en el namespace `dev` que podemos observar en la web de argoCD o en bash.

**VerificaciÃ³n:**
```bash
kubectl get pods -n dev

#salida : 
NAME                          READY   STATUS    RESTARTS      AGE
dev-webapp-7c9c8c8499-67cv4   1/1     Running   0             59s
dev-webapp-7c9c8c8499-xfr62   1/1     Running   1 (18m ago)   4d22h
```
DeberÃ­as ver dos pods corriendo para `dev-webapp`.


---

### 5.2 Prueba de auto-reparaciÃ³n GitOps

**AcciÃ³n:** Se escala manualmente el deployment a 0 rÃ©plicas (fuera de Git) y se observa cÃ³mo ArgoCD restaura el estado declarado en Git.

**Comandos:**
```bash
kubectl scale deployment dev-webapp -n dev --replicas=0
kubectl get pods -n dev -w

#Salida
NAME                          READY   STATUS        RESTARTS      AGE
dev-webapp-7c9c8c8499-67cv4   1/1     Terminating   0             2m13s
dev-webapp-7c9c8c8499-xfr62   1/1     Terminating   1 (19m ago)   4d22h
dev-webapp-7c9c8c8499-ww67f   0/1     Pending       0             0s
dev-webapp-7c9c8c8499-ww67f   0/1     Pending       0             0s
dev-webapp-7c9c8c8499-btz5z   0/1     Pending       0             0s
dev-webapp-7c9c8c8499-btz5z   0/1     Pending       0             0s
dev-webapp-7c9c8c8499-ww67f   0/1     ContainerCreating   0             0s
dev-webapp-7c9c8c8499-btz5z   0/1     ContainerCreating   0             0s
dev-webapp-7c9c8c8499-xfr62   0/1     Completed           1 (19m ago)   4d22h
dev-webapp-7c9c8c8499-67cv4   0/1     Completed           0             2m13s
dev-webapp-7c9c8c8499-67cv4   0/1     Completed           0             2m14s
dev-webapp-7c9c8c8499-67cv4   0/1     Completed           0             2m14s
dev-webapp-7c9c8c8499-xfr62   0/1     Completed           1 (19m ago)   4d22h
dev-webapp-7c9c8c8499-xfr62   0/1     Completed           1 (19m ago)   4d22h
dev-webapp-7c9c8c8499-ww67f   0/1     Running             0             3s
dev-webapp-7c9c8c8499-btz5z   0/1     Running             0             5s
dev-webapp-7c9c8c8499-ww67f   1/1     Running             0             9s
dev-webapp-7c9c8c8499-btz5z   1/1     Running             0             11s
```

**ExplicaciÃ³n:** Al modificar el estado del clÃºster manualmente, ArgoCD detecta la desviaciÃ³n y restaura el nÃºmero de rÃ©plicas a 2, cumpliendo el principio de GitOps: Git es la Ãºnica fuente de verdad.

---

### 5.3 Agregar un ConfigMap y parametrizaciÃ³n por entorno

**AcciÃ³n:** Se crea `base/configmap.yaml` con configuraciÃ³n genÃ©rica, se actualiza `base/kustomization.yaml` para incluir el ConfigMap y se modifica `base/deployment.yaml` para consumir el ConfigMap como variables de entorno.

**Comandos:**
```bash
cat > base/configmap.yaml << 'EOF'
apiVersion: v1
kind: ConfigMap
metadata:
  name: webapp-config  # nombre del mapa para usar en deployment.yaml
data:
  APP_ENV: "base"
  LOG_LEVEL: "info"
  FEATURE_FLAG: "disabled"
EOF

cat > base/kustomization.yaml << 'EOF'
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - deployment.yaml
  - service.yaml
  - configmap.yaml
commonLabels:
  app: webapp
  managed-by: argocd
EOF

cat > base/deployment.yaml << 'EOF'
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webapp
  labels:
    app: webapp
spec:
  replicas: 2
  selector:
    matchLabels:
      app: webapp
  template:
    metadata:
      labels:
        app: webapp
    spec:
      containers:
      - name: webapp
        image: nginxdemos/hello:latest
        ports:
        - containerPort: 80
        envFrom:
        - configMapRef:
            name: webapp-config  # Referencia al mapa creado
        resources:
          requests:
            memory: "64Mi"
            cpu: "50m"
          limits:
            memory: "128Mi"
            cpu: "100m"
        livenessProbe:
          httpGet:
            path: /
            port: 80
          initialDelaySeconds: 5
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /
            port: 80
          initialDelaySeconds: 5
          periodSeconds: 5
EOF
```

**ExplicaciÃ³n:** El ConfigMap permite parametrizar la aplicaciÃ³n segÃºn el entorno. Se aÃ±ade como recurso base y se referencia en el deployment para inyectar variables de entorno.

---

### 5.4 ConfiguraciÃ³n especÃ­fica para Dev


**AcciÃ³n:** Se crea `overlays/dev/configmap-patch.yaml` con valores especÃ­ficos para dev y se actualiza `overlays/dev/kustomization.yaml` para aplicar el parche.

**Comandos:**
```bash
cat > overlays/dev/configmap-patch.yaml << 'EOF'
apiVersion: v1
kind: ConfigMap
metadata:
  name: webapp-config  # emplea el kind y el name para saber donde a
data:
  APP_ENV: "development"
  LOG_LEVEL: "debug"
  FEATURE_FLAG: "enabled"
EOF

cat > overlays/dev/kustomization.yaml << 'EOF'
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
namespace: dev
namePrefix: dev-
bases:
  - ../../base
patchesStrategicMerge:
  - replica-patch.yaml
  - configmap-patch.yaml
commonLabels:
  environment: dev
EOF

git add .
git commit -m "Add ConfigMap with environment-specific configuration"
git push origin main
```

**VerificaciÃ³n:**
```bash
kubectl get configmaps -n dev
kubectl describe configmap dev-webapp-config -n dev
```
Deberiamos ver el ConfigMap con los valores especÃ­ficos para dev.

---
## Parte 6: PromociÃ³n de Cambios
### 6.1 PromociÃ³n a Staging

DespuÃ©s de validar los cambios en dev, promovimos el ConfigMap a staging siguiendo los pasos de la guÃ­a:

**Comandos ejecutados:**
```bash
cp overlays/dev/configmap-patch.yaml overlays/staging/configmap-patch.yaml
cat > overlays/staging/configmap-patch.yaml << 'EOF'
apiVersion: v1
kind: ConfigMap
metadata:
  name: webapp-config
data:
  APP_ENV: "staging"
  LOG_LEVEL: "info"
  FEATURE_FLAG: "enabled"
EOF

cat > overlays/staging/kustomization.yaml << 'EOF'
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
namespace: staging
namePrefix: staging-
bases:
  - ../../base
patchesStrategicMerge:
  - replica-patch.yaml
  - configmap-patch.yaml
commonLabels:
  environment: staging
EOF

git checkout -b promote-to-staging
git add overlays/staging/
git commit -m "Promote ConfigMap changes to staging"
git push origin promote-to-staging
```

**ExplicaciÃ³n:**
Se copia y adapta el parche de configuraciÃ³n para staging, se actualiza el kustomization, se crea una rama de promociÃ³n y se suben los cambios. En GitHub se crea y fusiona el Pull Request.

**SincronizaciÃ³n y descubrimiento del error:**
Tras hacer `git pull` en local y ejecutar:
```bash
kubectl get pods -n staging -w
kubectl get configmap -n staging
```
no aparecÃ­an recursos en staging. Descubrimos que faltaba aplicar el manifiesto de la aplicaciÃ³n ArgoCD para staging (`argocd-app-staging.yaml`).

**CorrecciÃ³n:**
Ejecutamos:
```bash
kubectl apply -f argocd-app-staging.yaml
```
Esto permitiÃ³ que ArgoCD gestionara el entorno staging y sincronizara los recursos correctamente.

**LecciÃ³n aprendida:**
Siempre hay que asegurarse de que los manifiestos de las aplicaciones ArgoCD estÃ©n aplicados antes de promover y verificar cambios en cada entorno.

A partir de aquÃ­, continuamos con la validaciÃ³n y despliegue en staging segÃºn la guÃ­a.


### 6.2 PromociÃ³n a ProducciÃ³n â€” Primera fase (preparaciÃ³n y push de la rama)

**Acciones ejecutadas:**

1) Copiar el parche de `staging` a `prod` y ajustar los valores para producciÃ³n:

```bash
cp overlays/staging/configmap-patch.yaml overlays/prod/configmap-patch.yaml
cat > overlays/prod/configmap-patch.yaml << 'EOF'
apiVersion: v1
kind: ConfigMap
metadata:
  name: webapp-config
data:
  APP_ENV: "production"
  LOG_LEVEL: "warn"
  FEATURE_FLAG: "enabled"
EOF 
```

2) Actualizar el `kustomization` de `prod` para referenciar la base y los parches adecuados:

```bash
cat > overlays/prod/kustomization.yaml << 'EOF'
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

namespace: prod
namePrefix: prod-

bases:
  - ../../base

patchesStrategicMerge:
  - replica-patch.yaml
  - configmap-patch.yaml

commonLabels:
  environment: prod
EOF
```

3) Crear la rama de promociÃ³n, commitear y subirla al remoto:

```bash
git checkout -b promote-to-prod
git add overlays/prod/
git commit -m "Promote ConfigMap to production"
git push origin promote-to-prod
```

**Salida observada:**

- Rama creada localmente: `promote-to-prod`.
- Commit aplicado (2 archivos modificados / 1 creado).
- Push completado al remoto. GitHub ofreciÃ³ la URL para crear el Pull Request:

  https://github.com/ignrui/k8s-gitops-demo/pull/new/promote-to-prod

**ExplicaciÃ³n:**

En esta primera fase los cambios de configuraciÃ³n para `prod` se preparan y se colocan en una rama de promociÃ³n. No se realiza ninguna sincronizaciÃ³n automÃ¡tica con el clÃºster en este paso: la rama solo contiene la propuesta de cambio que debe revisarse mediante un Pull Request y, una vez aprobada y fusionada a `main`, se procederÃ¡ a la sincronizaciÃ³n manual en ArgoCD (segÃºn la polÃ­tica de producciÃ³n definida).

Siguiente acciÃ³n: crear y revisar el Pull Request en GitHub; tras la fusiÃ³n a `main` ejecutar la sincronizaciÃ³n manual en ArgoCD para desplegar en producciÃ³n.

### 6.2 PromociÃ³n a ProducciÃ³n â€” SincronizaciÃ³n manual y resultado

**Acciones ejecutadas:**

```bash
git checkout main
git pull origin main

# Production won't auto-sync, must approve manually
argocd app sync webapp-prod

# Verify
kubectl get configmap -n prod
kubectl describe configmap prod-webapp-config -n prod
```

**Salida observada:**

- `git checkout` y `git pull` confirmaron que `main` estÃ¡ actualizado.
- `argocd app sync webapp-prod` devolviÃ³ un error de autenticaciÃ³n por token expirado (respuesta JSON):

  {"level":"fatal","msg":"rpc error: code = Unauthenticated desc = invalid session: token has invalid claims: token is expired","time":"2025-11-05T20:13:51+01:00"}

- ComprobaciÃ³n con `kubectl` mostrÃ³ que existe un `ConfigMap` en `prod` pero con los valores anteriores (no los valores de `production` reciÃ©n preparados):

  NAME                 DATA   AGE
  kube-root-ca.crt     1      7d9h
  prod-webapp-config   3      2d10h

  Data
  ====
  FEATURE_FLAG:
  ----
  disabled
  LOG_LEVEL:
  ----
  info
  APP_ENV:
  ----
  base

**ExplicaciÃ³n:**

El intento de sincronizaciÃ³n manual con `argocd` fallÃ³ por una sesiÃ³n/credencial caducada en el cliente (`argocd` CLI). Por tanto, ArgoCD no recibiÃ³ la orden de sincronizar desde la CLI y el clÃºster sigue mostrando el `ConfigMap` anterior (valores base). El push de la rama `promote-to-prod` ya existÃ­a en remoto, pero hasta que la rama no se fusione en `main` y se ejecute una sincronizaciÃ³n manual (o la aplicaciÃ³n sea actualizada desde la UI/CLI con sesiÃ³n vÃ¡lida), los cambios no se aplicarÃ¡n en el clÃºster de producciÃ³n.

**AcciÃ³n recomendada inmediata:**

- Renovar la sesiÃ³n de `argocd` (login) o usar la UI para realizar la sincronizaciÃ³n manual, y volver a ejecutar `argocd app sync webapp-prod`. Tras una sincronizaciÃ³n correcta, verificar con `kubectl describe configmap prod-webapp-config -n prod` que los valores `APP_ENV`, `LOG_LEVEL` y `FEATURE_FLAG` muestran los valores de `production`.

### Reintento: renovaciÃ³n de sesiÃ³n y sincronizaciÃ³n (salida real)

**Comandos ejecutados (sesiÃ³n renovada con contraseÃ±a manual):**

```bash
argocd login localhost:8080 --username admin --password ******** --insecure
argocd app sync webapp-prod
kubectl describe configmap prod-webapp-config -n prod
```

**Salida observada:**

- Login: `'admin:login' logged in successfully` y `Context 'localhost:8080' updated`.
- `argocd app sync webapp-prod` devolviÃ³ un resumen de operaciÃ³n indicando que la aplicaciÃ³n `argocd/webapp-prod` fue sincronizada con Ã©xito contra `main` (Sync Status: `Synced to main (b1cdbf7)`), con `Phase: Succeeded` y `Message: successfully synced (all tasks run)`.
- Detalle de recursos sincronizados (ejemplos):

  TIMESTAMP                  GROUP        KIND   NAMESPACE                  NAME    STATUS   HEALTH
  2025-11-05T20:18:15+01:00          ConfigMap        prod    prod-webapp-config    Synced
  2025-11-05T20:18:15+01:00            Service        prod   prod-webapp-service    Synced  Healthy
  2025-11-05T20:18:15+01:00   apps  Deployment        prod           prod-webapp    Synced  Healthy

- `kubectl describe configmap prod-webapp-config -n prod` mostrÃ³ que el `ConfigMap` en `prod` aÃºn contiene los valores anteriores (APP_ENV: `base`, LOG_LEVEL: `info`, FEATURE_FLAG: `disabled`).

**InterpretaciÃ³n y conclusiÃ³n:**

- El login y la sincronizaciÃ³n con ArgoCD fueron exitosos en la segunda tentativa: la operaciÃ³n de sincronizaciÃ³n se ejecutÃ³ y ArgoCD informÃ³ que la aplicaciÃ³n quedÃ³ `Synced` y `Healthy` (operaciÃ³n finalizada correctamente).
- Comprobamos el estado de las aplicaciones con la CLI de ArgoCD (`argocd app list` y `argocd app get`) y mediante acceso en el navegador. Las tres aplicaciones (`dev`, `staging` y `prod`) aparecen con estado "Synced" y salud "Healthy". En `prod` la sincronizaciÃ³n se ejecutÃ³ contra la revisiÃ³n `b4a0099` y ArgoCD reportÃ³ el `ConfigMap` como `configured` mientras que el `Deployment` y el `Service` quedaron `unchanged` (no se detectaron diferencias que ArgoCD aplicara). Tras la verificaciÃ³n en el clÃºster, el `ConfigMap` en `prod` seguÃ­a mostrando los valores previos, por lo que investigamos el historial de Git y la definiciÃ³n declarativa para identificar por quÃ© los cambios de configuraciÃ³n no se materializaron en los datos del `ConfigMap`.






## ğŸ”š Punto 10: Limpieza y desmontaje del entorno

Al finalizar las pruebas se procediÃ³ a limpiar el entorno local para dejar el sistema en un estado limpio. Los comandos ejecutados y la salida observada fueron los siguientes:

Comandos ejecutados:
```bash
pkill -f "port-forward"
# Delete ArgoCD applications
kubectl delete application webapp-dev -n argocd
kubectl delete application webapp-staging -n argocd
kubectl delete application webapp-prod -n argocd

# Delete namespaces
kubectl delete namespace dev staging prod

# Optional: Delete ArgoCD entirely
kubectl delete namespace argocd

# Stop Minikube
minikube stop

# Optional: Delete Minikube cluster (removes all data)
minikube delete
```

Salida observada (extracto):
```
application.argoproj.io "webapp-dev" deleted
[1]   Terminated              kubectl port-forward svc/dev-webapp-service 8081:80 -n dev > /tmp/portfwd-dev.log 2>&1
[2]-  Terminated              kubectl port-forward svc/staging-webapp-service 8082:80 -n staging > /tmp/portfwd-staging.log 2>&1
[3]+  Terminated              kubectl port-forward svc/prod-webapp-service 8083:80 -n prod > /tmp/portfwd-prod.log 2>&1
application.argoproj.io "webapp-staging" deleted
application.argoproj.io "webapp-prod" deleted
namespace "dev" deleted
namespace "staging" deleted
namespace "prod" deleted
namespace "argocd" deleted
âœ‹  Stopping node "minikube"  ...
ğŸ›‘  Powering off "minikube" via SSH ...
ğŸ›‘  1 node stopped.
ğŸ”¥  Deleting "minikube" in docker ...
ğŸ”¥  Deleting container "minikube" ...
ğŸ”¥  Removing /home/ctag/.minikube/machines/minikube ...
ğŸ’€  Removed all traces of the "minikube" cluster.
```

VerificaciÃ³n y notas:
- Se confirmaron la terminaciÃ³n de los `port-forward` (mensajes de proceso terminado) y la eliminaciÃ³n de las Applications de ArgoCD.
- Los namespaces `dev`, `staging`, `prod` y `argocd` fueron eliminados correctamente.
- `minikube stop` y `minikube delete` detuvieron y eliminaron el clÃºster local, incluyendo los contenedores asociados.
- Esta limpieza elimina todos los recursos creados durante el laboratorio; si necesita conservar artefactos o logs, hay que exportarlos antes de ejecutar estos comandos.

RecomendaciÃ³n final: antes de ejecutar la eliminaciÃ³n del clÃºster en entornos de producciÃ³n o con datos persistentes, confirmar copias de seguridad y exportar recursos crÃ­ticos (manifiestos, volÃºmenes, imÃ¡genes) para evitar pÃ©rdidas de informaciÃ³n.
